{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data Generation Tutorial using Phi-4, LLaMA, and Mixtral on RITS\n",
    "\n",
    "This tutorial demonstrates how to use SDG repository to generate synthetic question-answer pairs from documents using large language models like Phi-4 and LLaMA 3.3 70B. We will also generate data using Mixtral model for comparison. We'll cover:\n",
    "\n",
    "1. Setting up the environment\n",
    "2. Connecting to LLM servers\n",
    "3. Configuring the data generation pipeline\n",
    "4. Generating data with different models\n",
    "5. Comparing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable auto-reloading of modules - useful during development\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Instructions\n",
    "\n",
    "Before running this notebook, you'll need to:\n",
    "\n",
    "```bash \n",
    "pip install sdg-hub==0.1.0a4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install transformers\n",
    "%pip install protobuf sentencepiece  # for Mixtral-8x22B-Instruct-v0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "# datasets: For handling our data\n",
    "# OpenAI: For interfacing with the LLM servers\n",
    "# SDG components: For building our data generation pipeline\n",
    "from datasets import load_dataset, Dataset\n",
    "from openai import OpenAI\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from sdg_hub.flow import Flow\n",
    "from sdg_hub.pipeline import Pipeline\n",
    "from sdg_hub.sdg import SDG\n",
    "from sdg_hub.registry import PromptRegistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "timestamp = now.strftime('%Y%m%d-%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_ascii = True  # NOTE this is default\n",
    "# force_ascii = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_seed_data = False  # For production\n",
    "# sample_seed_data = True  # For test\n",
    "\n",
    "MAX_SEED_DATA = 1\n",
    "# MAX_SEED_DATA = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For production\n",
    "num_workers = 8   # Number of parallel workers\n",
    "batch_size = 8    # Batch size for processing\n",
    "save_freq = 1000  # How often to save checkpoints\n",
    "\n",
    "# For test\n",
    "# num_workers = 1   # Number of parallel workers\n",
    "# batch_size = 1    # Batch size for processing\n",
    "# save_freq = 1000  # How often to save checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup environments for [RITS](https://rits.fmaas.res.ibm.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "RITS_API_KEY = os.getenv(\"RITS_API_KEY\")\n",
    "# print(f\"RITS_API_KEY={RITS_API_KEY}\", flush=True)\n",
    "\n",
    "default_headers = {\"RITS_API_KEY\": RITS_API_KEY}\n",
    "\n",
    "url = \"https://rits.fmaas.res.ibm.com/ritsapi/inferenceinfo\"\n",
    "res = requests.get(url=url, headers=default_headers)\n",
    "assert res.status_code == 200\n",
    "model_list: list[dict[str, str]] = res.json()\n",
    "model_dict = { m[\"model_name\"]: m[\"endpoint\"] for m in model_list }\n",
    "\n",
    "def get_base_url(model_name: str)-> str:\n",
    "    endpoint = model_dict.get(model_name, \"http://0.0.0.0:8000\")  # fall back to vllm\n",
    "    return f\"{endpoint}/v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Seed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_name = \"samples\"\n",
    "# data_name = \"20250411_en_2\"\n",
    "# data_name = \"20250411_ja\"\n",
    "# data_name = \"20250411_ja_non_ascii\"\n",
    "data_name = \"teigaku-genzei\"\n",
    "\n",
    "seed_data_name = f\"seed_data_{data_name}\"\n",
    "seed_data_path = f\"{seed_data_name}.jsonl\"\n",
    "\n",
    "# import pandas as pd\n",
    "# df = pd.read_json(seed_data_path, orient='records', lines=True)\n",
    "# seed_data_path_non_ascii = f\"{seed_data_name}_non_ascii.jsonl\"\n",
    "# df.to_json(seed_data_path_non_ascii, orient='records', lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Create Seed Data from a [Test Case](https://github.com/Red-Hat-AI-Innovation-Team/sdg_hub/blob/a3f788bcc36702ef09bfee4be6e569d77ea8a20b/scripts/test_knowledge.py#L25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = [\n",
    "#     {\n",
    "#         \"icl_query_1\": \"what is the location of the tubal tonsils?\",\n",
    "#         \"icl_response_1\": \"The location of the tubal tonsils is the roof of the pharynx.\",\n",
    "#         \"icl_query_2\": \"How long does the adenoid grow?\",\n",
    "#         \"task_description\": \"Teaching about human anatomy, specifically tonsils\",\n",
    "#         \"icl_response_2\": \"The adenoid grows until the age of 5, starts to shrink at the age of 7 and becomes small in adulthood.\",\n",
    "#         \"icl_query_3\": \"What is the immune systems first line of defense against ingested or inhaled foreign pathogens?\",\n",
    "#         \"icl_response_3\": \"The tonsils are the immune systems first line of defense.\",\n",
    "#         \"document\": \"The **tonsils** are a set of lymphoid organs facing into the aerodigestive tract, which is known as Waldeyer's tonsillar ring and consists of the adenoid tonsil or pharyngeal tonsil, two tubal tonsils, two palatine tonsils, and the lingual tonsils. These organs play an important role in the immune system. When used unqualified, the term most commonly refers specifically to the palatine tonsils, which are two lymphoid organs situated at either side of the back of the human throat. The palatine tonsils and the adenoid tonsil are organs consisting of lymphoepithelial tissue located near the oropharynx and nasopharynx parts of the throat\",\n",
    "#         \"domain\": \"textbook\",\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# ds = Dataset.from_list(samples)\n",
    "# ds.to_json(seed_data_path, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Prepare Seed Data\n",
    "\n",
    "We'll load our seed data (documents) that will be used to generate question-answer pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the seed data from JSON file\n",
    "ds = load_dataset('json', data_files=seed_data_path, split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Reduce Seed Data for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sample_seed_data:\n",
    "    num_seed_data = len(ds)\n",
    "    num_seed_data = min(num_seed_data, MAX_SEED_DATA)\n",
    "\n",
    "    ds = ds.select(range(num_seed_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities for Generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_messages(generated_data: Dataset) -> Dataset:\n",
    "    messages_list: list[dict[str, any]] = []\n",
    "    for generated_datum in generated_data:\n",
    "        user = generated_datum['question']\n",
    "        assistant = generated_datum['response']\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": user},\n",
    "            {\"role\": \"assistant\", \"content\": assistant},\n",
    "        ]\n",
    "        messages_list.append({\"messages\": messages})\n",
    "    messages_data = Dataset.from_list(messages_list)\n",
    "    return messages_data\n",
    "\n",
    "def get_dataset_type(generated_data_i: dict[str, any]) -> str:\n",
    "    _dataset_type = generated_data_i.get('dataset_type', None)\n",
    "    if _dataset_type is not None:\n",
    "        _dataset_type = f\" ({_dataset_type})\"\n",
    "    else:\n",
    "        _dataset_type = \"\"\n",
    "    return _dataset_type\n",
    "\n",
    "def write_input(f, generated_data_i) -> None:\n",
    "    icl_document = generated_data_i.get('icl_document', None)\n",
    "    if icl_document is not None:\n",
    "        f.write(f\"### In-Context Learning Example\\n\\n\")\n",
    "        f.write(f\"#### ICL Document\\n\")\n",
    "        f.write(icl_document + \"\\n\\n\")\n",
    "    icl_query_1 = generated_data_i.get('icl_query_1', None)\n",
    "    if icl_query_1 is not None:\n",
    "        f.write(f\"#### ICL Query 1\\n\")\n",
    "        f.write(icl_query_1 + \"\\n\\n\")\n",
    "    icl_response_1 = generated_data_i.get('icl_response_1', None)\n",
    "    if icl_response_1 is not None:\n",
    "        f.write(f\"#### ICL Response 1\\n\")\n",
    "        f.write(icl_response_1 + \"\\n\\n\")\n",
    "    icl_query_2 = generated_data_i.get('icl_query_2', None)\n",
    "    if icl_query_2 is not None:\n",
    "        f.write(f\"#### ICL Query 2\\n\")\n",
    "        f.write(icl_query_2 + \"\\n\\n\")\n",
    "    icl_response_2 = generated_data_i.get('icl_response_2', None)\n",
    "    if icl_response_2 is not None:\n",
    "        f.write(f\"#### ICL Response 2\\n\")\n",
    "        f.write(icl_response_2 + \"\\n\\n\")\n",
    "    icl_query_3 = generated_data_i.get('icl_query_3', None)\n",
    "    if icl_query_3 is not None:\n",
    "        f.write(f\"#### ICL Query 3\\n\")\n",
    "        f.write(icl_query_3 + \"\\n\\n\")\n",
    "    icl_response_3 = generated_data_i.get('icl_response_3', None)\n",
    "    if icl_response_3 is not None:\n",
    "        f.write(f\"#### ICL Response 3\\n\")\n",
    "        f.write(icl_response_3 + \"\\n\\n\")\n",
    "    document_outline = generated_data_i.get('document_outline', None)\n",
    "    if document_outline is not None:\n",
    "        f.write(f\"### Document Outline\\n\")\n",
    "        f.write(document_outline + \"\\n\\n\")\n",
    "    raw_document = generated_data_i.get('raw_document', None)\n",
    "    if raw_document is not None:\n",
    "        f.write(f\"### Raw Document (not used for Q&A generation)\\n\")\n",
    "        f.write(raw_document + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_data_with_phi4 = True\n",
    "generate_data_with_phi4reasoningplus = False\n",
    "generate_data_with_llama3 = False\n",
    "generate_data_with_mixtral = False\n",
    "generate_data_with_mixtral8x22b = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDG with Phi-4 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Phi-4 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Phi-4 model running on RITS\n",
    "phi4_teacher_model = \"microsoft/phi-4\"\n",
    "phi4_endpoint = get_base_url(phi4_teacher_model)\n",
    "\n",
    "phi4_client = OpenAI(\n",
    "    api_key=\"EMPTY\",\n",
    "    base_url=phi4_endpoint,\n",
    "    default_headers=default_headers,\n",
    ")\n",
    "\n",
    "# Verify connection to Phi-4 model\n",
    "print(f\"Connected to Phi-4 model: {phi4_teacher_model}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Phi-4 Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the Phi-4 chat template\n",
    "# This ensures proper formatting of prompts for the model\n",
    "\n",
    "phi4_teacher_model_hf = \"microsoft/phi-4\"\n",
    "\n",
    "# Load the tokenizer to get the chat template\n",
    "phi4_tokenizer = AutoTokenizer.from_pretrained(phi4_teacher_model_hf)\n",
    "\n",
    "# Register the chat template in our prompt registry\n",
    "@PromptRegistry.register(phi4_teacher_model)\n",
    "def phi4_chat_template():\n",
    "    return phi4_tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Phi-4 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create flow configuration for Phi-4\n",
    "flow_cfg_phi4 = Flow(phi4_client).get_flow_from_file(\"synth_knowledge1.5_phi4_rits.yaml\")\n",
    "\n",
    "# Initialize SDG pipeline for Phi-4\n",
    "sdg_phi4 = SDG(\n",
    "    [Pipeline(flow_cfg_phi4)],\n",
    "    num_workers=num_workers,\n",
    "    batch_size=batch_size,\n",
    "    save_freq=save_freq,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data with Phi-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_data_with_phi4:\n",
    "    # Generate data using Phi-4 model\n",
    "    generated_data_phi4 = sdg_phi4.generate(ds, checkpoint_dir=\"Tmp-checkpoint\")\n",
    "\n",
    "    generated_path_phi4 = f\"generated_data_{data_name}_{timestamp}_phi4.jsonl\"\n",
    "    generated_data_phi4.to_json(generated_path_phi4, orient=\"records\", lines=True, force_ascii=force_ascii)\n",
    "    print(f\"Data saved to {generated_path_phi4}\", flush=True)\n",
    "\n",
    "    # Save generated data in messages format for training\n",
    "    messages_data_phi4 = to_messages(generated_data_phi4)\n",
    "\n",
    "    messages_data_path_phi4 = f\"messages_data_{data_name}_{timestamp}_phi4.jsonl\"\n",
    "    messages_data_phi4.to_json(messages_data_path_phi4, orient=\"records\", lines=True, force_ascii=force_ascii)\n",
    "    print(f\"Messages data saved to {messages_data_path_phi4}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Generated Data with Phi-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_data_with_phi4:\n",
    "    # Save comparison results to markdown file\n",
    "    output_file = f\"model_output_{data_name}_{timestamp}_phi4.md\"\n",
    "\n",
    "    if 'generated_data_phi4' not in locals():\n",
    "        generated_data_phi4 = []\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        num_generated_data_phi4 = len(generated_data_phi4)\n",
    "\n",
    "        # Number of examples to compare\n",
    "        k = num_generated_data_phi4\n",
    "\n",
    "        # Compare generated Q&A pairs\n",
    "        for i in range(k):\n",
    "            f.write(\"# Example #{}\\n\\n\".format(i+1))\n",
    "\n",
    "            if i < num_generated_data_phi4:\n",
    "                # Phi-4 results\n",
    "                write_input(f, generated_data_phi4[i])\n",
    "                f.write(f\"### Document{get_dataset_type(generated_data_phi4[i])} from phi-4\\n\")\n",
    "                f.write(generated_data_phi4[i]['document'] + \"\\n\\n\")\n",
    "                f.write(\"### Result from phi-4\\n\")\n",
    "                f.write(generated_data_phi4[i]['question'] + \"\\n\")\n",
    "                f.write(\"*******************************\\n\")\n",
    "                f.write(generated_data_phi4[i]['response'] + \"\\n\")\n",
    "\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    print(f\"Wrote {k} examples to {output_file}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) SDG with Phi-4-reasoning-plus Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Phi-4-reasoning-plus Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Phi-4-reasoning-plus model running on vLLM\n",
    "phi4reasoningplus_teacher_model = \"microsoft/Phi-4-reasoning-plus\"\n",
    "phi4reasoningplus_endpoint = get_base_url(phi4reasoningplus_teacher_model)\n",
    "\n",
    "phi4reasoningplus_client = OpenAI(\n",
    "    api_key=\"EMPTY\",\n",
    "    base_url=phi4reasoningplus_endpoint,\n",
    "    default_headers=default_headers,\n",
    ")\n",
    "\n",
    "# Verify connection to Phi-4-reasoning-plus model\n",
    "print(f\"Connected to Phi-4-reasoning-plus model: {phi4reasoningplus_teacher_model}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Phi-4-reasoning-plus Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the Phi-4-reasoning-plus chat template\n",
    "# This ensures proper formatting of prompts for the model\n",
    "\n",
    "phi4reasoningplus_teacher_model_hf = \"microsoft/Phi-4-reasoning-plus\"\n",
    "\n",
    "# Load the tokenizer to get the chat template\n",
    "phi4reasoningplus_tokenizer = AutoTokenizer.from_pretrained(phi4reasoningplus_teacher_model_hf)\n",
    "\n",
    "# Register the chat template in our prompt registry\n",
    "@PromptRegistry.register(phi4reasoningplus_teacher_model)\n",
    "def phi4reasoningplus_chat_template():\n",
    "    # @@@ahoaho XXX\n",
    "    # chat_template = phi4reasoningplus_tokenizer.chat_template\n",
    "    # chat_template = \"<|im_start|>system<|im_sep|>You are Phi, a language model trained by Microsoft to help users. Your role as an assistant involves thoroughly exploring questions through a systematic thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracing, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution using the specified format: <think> {Thought section} </think> {Solution section}. In the Thought section, detail your reasoning process in steps. Each step should include detailed considerations such as analysing questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The Solution section should be logical, accurate, and concise and detail necessary steps needed to reach the conclusion. Now, try to solve the following question through the above guidelines:<|im_end|>{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|im_start|>user<|im_sep|>' + message['content'] + '<|im_end|>'}}{% elif (message['role'] == 'assistant') %}{{'<|im_start|>assistant<|im_sep|>'}}{% generation %}{{message['content'] + '<|im_end|>'}}{% endgeneration %}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant<|im_sep|>' }}{% endif %}\"\n",
    "    # NOTE removed \"generation\" and \"endgeneration\" tags from the original template\n",
    "    chat_template = \"<|im_start|>system<|im_sep|>You are Phi, a language model trained by Microsoft to help users. Your role as an assistant involves thoroughly exploring questions through a systematic thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracing, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution using the specified format: <think> {Thought section} </think> {Solution section}. In the Thought section, detail your reasoning process in steps. Each step should include detailed considerations such as analysing questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The Solution section should be logical, accurate, and concise and detail necessary steps needed to reach the conclusion. Now, try to solve the following question through the above guidelines:<|im_end|>{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|im_start|>user<|im_sep|>' + message['content'] + '<|im_end|>'}}{% elif (message['role'] == 'assistant') %}{{'<|im_start|>assistant<|im_sep|>' + message['content'] + '<|im_end|>'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant<|im_sep|>' }}{% endif %}\"\n",
    "    return chat_template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Phi-4-reasoning-plus Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create flow configuration for Phi-4-reasoning-plus\n",
    "flow_cfg_phi4reasoningplus = Flow(phi4reasoningplus_client).get_flow_from_file(\"synth_knowledge1.5_phi4reasoningplus.yaml\")\n",
    "\n",
    "# Initialize SDG pipeline for Phi-4-reasoning-plus\n",
    "sdg_phi4reasoningplus = SDG(\n",
    "    [Pipeline(flow_cfg_phi4reasoningplus)],\n",
    "    num_workers=num_workers,\n",
    "    batch_size=batch_size,\n",
    "    save_freq=save_freq,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data with Phi-4-reasoning-plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_data_with_phi4reasoningplus:\n",
    "    # Generate data using Phi-4-reasoning-plus model\n",
    "    generated_data_phi4reasoningplus = sdg_phi4reasoningplus.generate(ds, checkpoint_dir=\"Tmp-checkpoint\")\n",
    "\n",
    "    generated_path_phi4reasoningplus = f\"generated_data_{data_name}_{timestamp}_phi4reasoningplus.jsonl\"\n",
    "    generated_data_phi4reasoningplus.to_json(generated_path_phi4reasoningplus, orient=\"records\", lines=True, force_ascii=force_ascii)\n",
    "    print(f\"Data saved to {generated_path_phi4reasoningplus}\", flush=True)\n",
    "\n",
    "    # Save generated data in messages format for training\n",
    "    messages_data_phi4reasoningplus = to_messages(generated_data_phi4reasoningplus)\n",
    "\n",
    "    messages_data_path_phi4reasoningplus = f\"messages_data_{data_name}_{timestamp}_phi4reasoningplus.jsonl\"\n",
    "    messages_data_phi4reasoningplus.to_json(messages_data_path_phi4reasoningplus, orient=\"records\", lines=True, force_ascii=force_ascii)\n",
    "    print(f\"Messages data saved to {messages_data_path_phi4reasoningplus}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Generated Data with Phi-4-reasoning-plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_data_with_phi4reasoningplus:\n",
    "    # Save comparison results to markdown file\n",
    "    output_file = f\"model_output_{data_name}_{timestamp}_phi4reasoningplus.md\"\n",
    "\n",
    "    if 'generated_data_phi4reasoningplus' not in locals():\n",
    "        generated_data_phi4reasoningplus = []\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        num_generated_data_phi4reasoningplus = len(generated_data_phi4reasoningplus)\n",
    "\n",
    "        # Number of examples to compare\n",
    "        k = num_generated_data_phi4reasoningplus\n",
    "\n",
    "        # Compare generated Q&A pairs\n",
    "        for i in range(k):\n",
    "            f.write(\"# Example #{}\\n\\n\".format(i+1))\n",
    "\n",
    "            if i < num_generated_data_phi4reasoningplus:\n",
    "                # Phi-4-reasoning-plus results\n",
    "                write_input(f, generated_data_phi4reasoningplus[i])\n",
    "                f.write(f\"### Document{get_dataset_type(generated_data_phi4reasoningplus[i])} from Phi-4-reasoning-plus\\n\")\n",
    "                f.write(generated_data_phi4reasoningplus[i]['document'] + \"\\n\\n\")\n",
    "                f.write(\"### Result from Phi-4-reasoning-plus\\n\")\n",
    "                f.write(generated_data_phi4reasoningplus[i]['question'] + \"\\n\")\n",
    "                f.write(\"*******************************\\n\")\n",
    "                f.write(generated_data_phi4reasoningplus[i]['response'] + \"\\n\")\n",
    "\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    print(f\"Wrote {k} examples to {output_file}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) SDG with LLaMA 3.3 70B Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up LLaMA 3.3 70B Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure OpenAI client to connect to RITS server\n",
    "llama3_teacher_model = \"meta-llama/llama-3-3-70b-instruct\"\n",
    "llama3_endpoint = get_base_url(llama3_teacher_model)\n",
    "\n",
    "llama3_client = OpenAI(\n",
    "    api_key=\"EMPTY\",\n",
    "    base_url=llama3_endpoint,\n",
    "    default_headers=default_headers,\n",
    ")\n",
    "\n",
    "print(f\"Connected to Llama-3.3 model: {llama3_teacher_model}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure LLaMA 3.3 Prompt Template\n",
    "\n",
    "We need to register the correct chat template for our model to ensure proper prompt formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the LLaMA 3.3 chat template\n",
    "# This ensures proper formatting of prompts for the model\n",
    "\n",
    "# llama3_teacher_model_hf = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "llama3_teacher_model_hf = \"unsloth/Llama-3.3-70B-Instruct\"\n",
    "\n",
    "# Load the tokenizer to get the chat template\n",
    "llama3_tokenizer = AutoTokenizer.from_pretrained(llama3_teacher_model_hf)\n",
    "\n",
    "# Register the chat template in our prompt registry\n",
    "@PromptRegistry.register(llama3_teacher_model)\n",
    "def llama3_chat_template():\n",
    "    return llama3_tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the Data Generation Pipeline\n",
    "\n",
    "Now we'll set up our Synthetic Data Generation (SDG) pipeline with the following components:\n",
    "1. SDG Flow configuration from YAML\n",
    "2. SDG Pipeline setup\n",
    "3. SDG configuration with batch processing, number of workers, and save frequency parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the flow configuration from YAML file\n",
    "flow_cfg_llama3 = Flow(llama3_client).get_flow_from_file(\"synth_knowledge1.5_llama3.3_rits.yaml\")\n",
    "\n",
    "# Initialize the SDG pipeline with processing parameters\n",
    "sdg_llama3 = SDG(\n",
    "    [Pipeline(flow_cfg_llama3)],\n",
    "    num_workers=num_workers,\n",
    "    batch_size=batch_size,\n",
    "    save_freq=save_freq,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data with LLaMA 3.3\n",
    "\n",
    "Now we'll use our configured pipeline to generate synthetic question-answer pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_data_with_llama3:\n",
    "    # Generate synthetic data and save checkpoints\n",
    "    generated_data_llama3 = sdg_llama3.generate(ds, checkpoint_dir=\"Tmp-checkpoint\")\n",
    "\n",
    "    generated_path_llama3 = f\"generated_data_{data_name}_{timestamp}_llama3.jsonl\"\n",
    "    generated_data_llama3.to_json(generated_path_llama3, orient=\"records\", lines=True, force_ascii=force_ascii)\n",
    "    print(f\"Data saved to {generated_path_llama3}\", flush=True)\n",
    "\n",
    "    # Save generated data in messages format for training\n",
    "    messages_data_llama3 = to_messages(generated_data_llama3)\n",
    "\n",
    "    messages_data_path_llama3 = f\"messages_data_{data_name}_{timestamp}_llama3.jsonl\"\n",
    "    messages_data_llama3.to_json(messages_data_path_llama3, orient=\"records\", lines=True, force_ascii=force_ascii)\n",
    "    print(f\"Messages data saved to {messages_data_path_llama3}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Generated Data with LLaMA 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_data_with_llama3:\n",
    "    # Save comparison results to markdown file\n",
    "    output_file = f\"model_output_{data_name}_{timestamp}_llama3.md\"\n",
    "\n",
    "    if 'generated_data_llama3' not in locals():\n",
    "        generated_data_llama3 = []\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        num_generated_data_llama3 = len(generated_data_llama3)\n",
    "\n",
    "        # Number of examples to compare\n",
    "        k = num_generated_data_llama3\n",
    "\n",
    "        # Compare generated Q&A pairs\n",
    "        for i in range(k):\n",
    "            f.write(\"# Example #{}\\n\\n\".format(i+1))\n",
    "\n",
    "            if i < num_generated_data_llama3:\n",
    "                # LLaMA 3.3 results\n",
    "                write_input(f, generated_data_llama3[i])\n",
    "                f.write(f\"### Document{get_dataset_type(generated_data_llama3[i])} from llama-3.3-70b\\n\")\n",
    "                f.write(generated_data_llama3[i]['document'] + \"\\n\\n\")\n",
    "                f.write(\"### Result from llama-3.3-70b\\n\")\n",
    "                f.write(generated_data_llama3[i]['question'] + \"\\n\")\n",
    "                f.write(\"*******************************\\n\")\n",
    "                f.write(generated_data_llama3[i]['response'] + \"\\n\")\n",
    "\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    print(f\"Wrote {k} examples to {output_file}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) SDG with Mixtral-8x7B Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Mixtral-8x7B Model\n",
    "\n",
    "For comparison, we'll also generate data using the Mixtral model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Mixtral model running on RITS\n",
    "mixtral_teacher_model = \"mistralai/mixtral-8x7B-instruct-v0.1\"\n",
    "mixtral_endpoint = get_base_url(mixtral_teacher_model)\n",
    "\n",
    "mixtral_client = OpenAI(\n",
    "    api_key=\"EMPTY\",\n",
    "    base_url=mixtral_endpoint,\n",
    "    default_headers=default_headers,\n",
    ")\n",
    "\n",
    "# Verify connection to Mixtral model\n",
    "print(f\"Connected to Mixtral model: {mixtral_teacher_model}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Mixtral-8x7B Prompt Template\n",
    "\n",
    "We need to register the correct chat template for our model to ensure proper prompt formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the Mixtral chat template\n",
    "# This ensures proper formatting of prompts for the model\n",
    "\n",
    "mixtral_teacher_model_hf = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "# Load the tokenizer to get the chat template\n",
    "mixtral_tokenizer = AutoTokenizer.from_pretrained(mixtral_teacher_model_hf)\n",
    "\n",
    "# Register the chat template in our prompt registry\n",
    "@PromptRegistry.register(mixtral_teacher_model)\n",
    "def mixtral_chat_template():\n",
    "    return mixtral_tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Mixtral-8x7B Pipeline\n",
    "\n",
    "Set up a similar pipeline for Mixtral model generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create flow configuration for Mixtral\n",
    "flow_cfg_mixtral = Flow(mixtral_client).get_flow_from_file(\"synth_knowledge1.5_mixtral_rits.yaml\")\n",
    "\n",
    "# Initialize SDG pipeline for Mixtral\n",
    "sdg_mixtral = SDG(\n",
    "    [Pipeline(flow_cfg_mixtral)],\n",
    "    num_workers=num_workers,\n",
    "    batch_size=batch_size,\n",
    "    save_freq=save_freq,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data with Mixtral-8x7B\n",
    "\n",
    "Generate synthetic data using the Mixtral model for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_data_with_mixtral:\n",
    "    # Generate data using Mixtral model\n",
    "    generated_data_mixtral = sdg_mixtral.generate(ds, checkpoint_dir=\"Tmp-checkpoint\")\n",
    "\n",
    "    generated_path_mixtral = f\"generated_data_{data_name}_{timestamp}_mixtral.jsonl\"\n",
    "    generated_data_mixtral.to_json(generated_path_mixtral, orient=\"records\", lines=True, force_ascii=force_ascii)\n",
    "    print(f\"Data saved to {generated_path_mixtral}\", flush=True)\n",
    "\n",
    "    # Save generated data in messages format for training\n",
    "    messages_data_mixtral = to_messages(generated_data_mixtral)\n",
    "\n",
    "    messages_data_path_mixtral = f\"messages_data_{data_name}_{timestamp}_mixtral.jsonl\"\n",
    "    messages_data_mixtral.to_json(messages_data_path_mixtral, orient=\"records\", lines=True, force_ascii=force_ascii)\n",
    "    print(f\"Messages data saved to {messages_data_path_mixtral}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Generated Data with Mixtral-8x7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_data_with_mixtral:\n",
    "    # Save comparison results to markdown file\n",
    "    output_file = f\"model_output_{data_name}_{timestamp}_mixtral.md\"\n",
    "\n",
    "    if 'generated_data_mixtral' not in locals():\n",
    "        generated_data_mixtral = []\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        num_generated_data_mixtral = len(generated_data_mixtral)\n",
    "\n",
    "        # Number of examples to compare\n",
    "        k = num_generated_data_mixtral\n",
    "\n",
    "        # Compare generated Q&A pairs\n",
    "        for i in range(k):\n",
    "            f.write(\"# Example #{}\\n\\n\".format(i+1))\n",
    "\n",
    "            if i < num_generated_data_mixtral:\n",
    "                # Mixtral-8x7B results\n",
    "                write_input(f, generated_data_mixtral[i])\n",
    "                f.write(f\"### Document{get_dataset_type(generated_data_mixtral[i])} from mixtral-8x7B\\n\")\n",
    "                f.write(generated_data_mixtral[i]['document'] + \"\\n\\n\")\n",
    "                f.write(\"### Result from mixtral-8x7B\\n\")\n",
    "                f.write(generated_data_mixtral[i]['question'] + \"\\n\")\n",
    "                f.write(\"*******************************\\n\")\n",
    "                f.write(generated_data_mixtral[i]['response'] + \"\\n\")\n",
    "\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    print(f\"Wrote {k} examples to {output_file}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) SDG with Mixtral-8x22B Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Mixtral-8x22B Model\n",
    "\n",
    "For comparison, we'll also generate data using the Mixtral model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Mixtral model running on RITS\n",
    "mixtral8x22b_teacher_model = \"mistralai/mixtral-8x22B-instruct-v0.1\"\n",
    "mixtral8x22b_endpoint = get_base_url(mixtral8x22b_teacher_model)\n",
    "\n",
    "mixtral8x22b_client = OpenAI(\n",
    "    api_key=\"EMPTY\",\n",
    "    base_url=mixtral8x22b_endpoint,\n",
    "    default_headers=default_headers,\n",
    ")\n",
    "\n",
    "# Verify connection to Mixtral model\n",
    "print(f\"Connected to Mixtral model: {mixtral8x22b_teacher_model}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Mixtral-8x22B Prompt Template\n",
    "\n",
    "We need to register the correct chat template for our model to ensure proper prompt formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the Mixtral chat template\n",
    "# This ensures proper formatting of prompts for the model\n",
    "\n",
    "mixtral8x22b_teacher_model_hf = \"mistralai/Mixtral-8x22B-Instruct-v0.1\"\n",
    "\n",
    "# Load the tokenizer to get the chat template\n",
    "mixtral8x22b_tokenizer = AutoTokenizer.from_pretrained(mixtral8x22b_teacher_model_hf)\n",
    "\n",
    "# Register the chat template in our prompt registry\n",
    "@PromptRegistry.register(mixtral8x22b_teacher_model)\n",
    "def mixtral8x22b_chat_template():\n",
    "    return mixtral8x22b_tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Mixtral-8x22B Pipeline\n",
    "\n",
    "Set up a similar pipeline for Mixtral model generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create flow configuration for Mixtral\n",
    "flow_cfg_mixtral8x22b = Flow(mixtral8x22b_client).get_flow_from_file(\"synth_knowledge1.5_mixtral8x22b_rits.yaml\")\n",
    "\n",
    "# Initialize SDG pipeline for Mixtral\n",
    "sdg_mixtral8x22b = SDG(\n",
    "    [Pipeline(flow_cfg_mixtral8x22b)],\n",
    "    num_workers=num_workers,\n",
    "    batch_size=batch_size,\n",
    "    save_freq=save_freq,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data with Mixtral-8x22B\n",
    "\n",
    "Generate synthetic data using the Mixtral model for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_data_with_mixtral8x22b:\n",
    "    # Generate data using Mixtral model\n",
    "    generated_data_mixtral8x22b = sdg_mixtral8x22b.generate(ds, checkpoint_dir=\"Tmp-checkpoint\")\n",
    "\n",
    "    generated_path_mixtral8x22b = f\"generated_data_{data_name}_{timestamp}_mixtral8x22b.jsonl\"\n",
    "    generated_data_mixtral8x22b.to_json(generated_path_mixtral8x22b, orient=\"records\", lines=True, force_ascii=force_ascii)\n",
    "    print(f\"Data saved to {generated_path_mixtral8x22b}\", flush=True)\n",
    "\n",
    "    # Save generated data in messages format for training\n",
    "    messages_data_mixtral8x22b = to_messages(generated_data_mixtral8x22b)\n",
    "\n",
    "    messages_data_path_mixtral8x22b = f\"messages_data_{data_name}_{timestamp}_mixtral8x22b.jsonl\"\n",
    "    messages_data_mixtral8x22b.to_json(messages_data_path_mixtral8x22b, orient=\"records\", lines=True, force_ascii=force_ascii)\n",
    "    print(f\"Messages data saved to {messages_data_path_mixtral8x22b}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Generated Data with Mixtral-8x22B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_data_with_mixtral8x22b:\n",
    "    # Save comparison results to markdown file\n",
    "    output_file = f\"model_output_{data_name}_{timestamp}_mixtral8x22b.md\"\n",
    "\n",
    "    if 'generated_data_mixtral8x22b' not in locals():\n",
    "        generated_data_mixtral8x22b = []\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        num_generated_data_mixtral8x22b = len(generated_data_mixtral8x22b)\n",
    "\n",
    "        # Number of examples to compare\n",
    "        k = num_generated_data_mixtral8x22b\n",
    "\n",
    "        # Compare generated Q&A pairs\n",
    "        for i in range(k):\n",
    "            f.write(\"# Example #{}\\n\\n\".format(i+1))\n",
    "\n",
    "            if i < num_generated_data_mixtral8x22b:\n",
    "                # Mixtral-8x22B results\n",
    "                write_input(f, generated_data_mixtral8x22b[i])\n",
    "                f.write(f\"### Document{get_dataset_type(generated_data_mixtral8x22b[i])} from mixtral-8x22B\\n\")\n",
    "                f.write(generated_data_mixtral8x22b[i]['document'] + \"\\n\\n\")\n",
    "                f.write(\"### Result from mixtral-8x22B\\n\")\n",
    "                f.write(generated_data_mixtral8x22b[i]['question'] + \"\\n\")\n",
    "                f.write(\"*******************************\\n\")\n",
    "                f.write(generated_data_mixtral8x22b[i]['response'] + \"\\n\")\n",
    "\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    print(f\"Wrote {k} examples to {output_file}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Generated Data\n",
    "\n",
    "Let's compare the outputs from both models by saving them to a markdown file for easy review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison results to markdown file\n",
    "output_file = f\"model_comparison_{data_name}_{timestamp}.md\"\n",
    "\n",
    "if 'generated_data_phi4' not in locals():\n",
    "    generated_data_phi4 = []\n",
    "\n",
    "if 'generated_data_phi4reasoningplus' not in locals():\n",
    "    generated_data_phi4reasoningplus = []\n",
    "\n",
    "if 'generated_data_llama3' not in locals():\n",
    "    generated_data_llama3 = []\n",
    "\n",
    "if 'generated_data_mixtral' not in locals():\n",
    "    generated_data_mixtral = []\n",
    "\n",
    "if 'generated_data_mixtral8x22b' not in locals():\n",
    "    generated_data_mixtral8x22b = []\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    num_generated_data_phi4 = len(generated_data_phi4)\n",
    "    num_generated_data_phi4reasoningplus = len(generated_data_phi4reasoningplus)\n",
    "    num_generated_data_llama3 = len(generated_data_llama3)\n",
    "    num_generated_data_mixtral = len(generated_data_mixtral)\n",
    "    num_generated_data_mixtral8x22b = len(generated_data_mixtral8x22b)\n",
    "\n",
    "    # Number of examples to compare\n",
    "    k = max(num_generated_data_phi4, num_generated_data_phi4reasoningplus, num_generated_data_llama3, num_generated_data_mixtral, num_generated_data_mixtral8x22b)\n",
    "\n",
    "    # Compare generated Q&A pairs\n",
    "    for i in range(k):\n",
    "        f.write(\"# Example #{}\\n\\n\".format(i+1))\n",
    "\n",
    "        if i < num_generated_data_phi4:\n",
    "            # Phi-4 results\n",
    "            write_input(f, generated_data_phi4[i])\n",
    "            f.write(f\"### Document{get_dataset_type(generated_data_phi4[i])} from phi-4\\n\")\n",
    "            f.write(generated_data_phi4[i]['document'] + \"\\n\\n\")\n",
    "            f.write(\"### Result from phi-4\\n\")\n",
    "            f.write(generated_data_phi4[i]['question'] + \"\\n\")\n",
    "            f.write(\"*******************************\\n\")\n",
    "            f.write(generated_data_phi4[i]['response'] + \"\\n\")\n",
    "\n",
    "        if i < num_generated_data_phi4reasoningplus:\n",
    "            # Phi-4-reasoning-plus results\n",
    "            write_input(f, generated_data_phi4reasoningplus[i])\n",
    "            f.write(f\"### Document{get_dataset_type(generated_data_phi4reasoningplus[i])} from Phi-4-reasoning-plus\\n\")\n",
    "            f.write(generated_data_phi4reasoningplus[i]['document'] + \"\\n\\n\")\n",
    "            f.write(\"### Result from Phi-4-reasoning-plus\\n\")\n",
    "            f.write(generated_data_phi4reasoningplus[i]['question'] + \"\\n\")\n",
    "            f.write(\"*******************************\\n\")\n",
    "            f.write(generated_data_phi4reasoningplus[i]['response'] + \"\\n\")\n",
    "\n",
    "        if i < num_generated_data_llama3:\n",
    "            # LLaMA 3.3 results\n",
    "            write_input(f, generated_data_llama3[i])\n",
    "            f.write(f\"### Document{get_dataset_type(generated_data_llama3[i])} from llama-3.3-70b\\n\")\n",
    "            f.write(generated_data_llama3[i]['document'] + \"\\n\\n\")\n",
    "            f.write(\"### Result from llama-3.3-70b\\n\")\n",
    "            f.write(generated_data_llama3[i]['question'] + \"\\n\")\n",
    "            f.write(\"*******************************\\n\")\n",
    "            f.write(generated_data_llama3[i]['response'] + \"\\n\")\n",
    "\n",
    "        if i < num_generated_data_mixtral:\n",
    "            # Mixtral-8x7B results\n",
    "            write_input(f, generated_data_mixtral[i])\n",
    "            f.write(f\"### Document{get_dataset_type(generated_data_mixtral[i])} from mixtral-8x7B\\n\")\n",
    "            f.write(generated_data_mixtral[i]['document'] + \"\\n\\n\")\n",
    "            f.write(\"### Result from mixtral-8x7B\\n\")\n",
    "            f.write(generated_data_mixtral[i]['question'] + \"\\n\")\n",
    "            f.write(\"*******************************\\n\")\n",
    "            f.write(generated_data_mixtral[i]['response'] + \"\\n\")\n",
    "\n",
    "        if i < num_generated_data_mixtral8x22b:\n",
    "            # Mixtral-8x22B results\n",
    "            write_input(f, generated_data_mixtral8x22b[i])\n",
    "            f.write(f\"### Document{get_dataset_type(generated_data_mixtral8x22b[i])} from mixtral-8x22B\\n\")\n",
    "            f.write(generated_data_mixtral8x22b[i]['document'] + \"\\n\\n\")\n",
    "            f.write(\"### Result from mixtral-8x22B\\n\")\n",
    "            f.write(generated_data_mixtral8x22b[i]['question'] + \"\\n\")\n",
    "            f.write(\"*******************************\\n\")\n",
    "            f.write(generated_data_mixtral8x22b[i]['response'] + \"\\n\")\n",
    "\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Wrote {k} examples to {output_file}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Usage\n",
    "\n",
    "For large-scale data generation, use the command-line script instead of this notebook:\n",
    "\n",
    "```bash\n",
    "python scripts/generate.py --ds_path seed_data.jsonl \\\n",
    "    --bs 2 --num_workers 10 \\\n",
    "    --save_path <your_save_path> \\\n",
    "    --flow ../src/sdg_hub/flows/generation/knowledge/synth_knowledge1.5.yaml \\\n",
    "    --checkpoint_dir <your_checkpoint_dir> \\\n",
    "    --endpoint <your_endpoint>\n",
    "```\n",
    "\n",
    "Note: For LLaMA 3.3, use `synth_knowledge1.5_llama3.3.yaml` as the flow configuration file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdg_hub-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
